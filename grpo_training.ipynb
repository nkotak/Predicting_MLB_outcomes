{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLB GRPO Models - Training and Evaluation\n",
    "\n",
    "This notebook implements Group Relative Policy Optimization (GRPO) models for MLB game prediction.\n",
    "\n",
    "## Models Implemented:\n",
    "1. **GRPO Classifier** - Neural network with GRPO-style training\n",
    "2. **GRPO Ensemble** - Ensemble of models with different feature subsets\n",
    "3. **GRPO Ranking Model** - Pairwise ranking approach for matchups\n",
    "4. **GRPO Betting Optimizer** - Model optimized for betting ROI\n",
    "\n",
    "## Enhanced Features:\n",
    "- Pitcher vs batter matchup features\n",
    "- Head-to-head team results\n",
    "- Situational features (RISP, clutch, etc.)\n",
    "- Pitcher 3-start rolling averages\n",
    "- MLB Advanced stats (45+ features)\n",
    "- Betting odds integration with ROI analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GRPO modules\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from grpo_models.feature_engineering import MLBFeatureEngineer, FeatureConfig, get_feature_columns\n",
    "from grpo_models.grpo_models import (\n",
    "    GRPOConfig, GRPOClassifier, GRPOEnsemble, \n",
    "    GRPORankingModel, GRPOBettingOptimizer\n",
    ")\n",
    "from grpo_models.betting_integration import (\n",
    "    BettingConfig, BettingAnalyzer, ROIAnalyzer,\n",
    "    add_synthetic_odds, generate_betting_report\n",
    ")\n",
    "from grpo_models.training_pipeline import (\n",
    "    TrainingConfig, DataPreparer, ModelTrainer,\n",
    "    BettingEvaluator, ResultsReporter\n",
    ")\n",
    "\n",
    "print(\"GRPO modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Note: Update these paths to match your data location\n",
    "batting_file = 'cleaned_batting_df.csv'  # Update path if needed\n",
    "pitching_file = 'cleaned_piching_df.csv'  # Update path if needed\n",
    "game_file = 'cleaned_game_df.csv'  # Update path if needed\n",
    "\n",
    "try:\n",
    "    batting_df = pd.read_csv(batting_file)\n",
    "    pitching_df = pd.read_csv(pitching_file)\n",
    "    game_df = pd.read_csv(game_file)\n",
    "    \n",
    "    # Clean up index columns\n",
    "    for df in [batting_df, pitching_df, game_df]:\n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"Batting data: {batting_df.shape}\")\n",
    "    print(f\"Pitching data: {pitching_df.shape}\")\n",
    "    print(f\"Game data: {game_df.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")\n",
    "    print(\"Please update the file paths above to point to your data files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data exploration\n",
    "print(\"Game DataFrame columns:\")\n",
    "print(game_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Apply enhanced feature engineering including:\n",
    "- Head-to-head features\n",
    "- Situational features (RISP, clutch hitting)\n",
    "- Pitcher 3-start rolling averages\n",
    "- Advanced team stats (Pythagorean expectation, run differential, streaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "feature_config = FeatureConfig(\n",
    "    rolling_windows=[3, 7, 14],\n",
    "    pitcher_rolling_starts=3,\n",
    "    include_advanced_stats=True,\n",
    "    include_betting_odds=True\n",
    ")\n",
    "\n",
    "feature_engineer = MLBFeatureEngineer(feature_config)\n",
    "\n",
    "print(\"Feature engineer initialized with config:\")\n",
    "print(f\"  Rolling windows: {feature_config.rolling_windows}\")\n",
    "print(f\"  Pitcher rolling starts: {feature_config.pitcher_rolling_starts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize team names\n",
    "batting_df.replace(to_replace='FLA', value='MIA', inplace=True)\n",
    "pitching_df.replace(to_replace='FLA', value='MIA', inplace=True)\n",
    "game_df.replace(to_replace='FLO', value='MIA', inplace=True)\n",
    "\n",
    "# Additional standardization for game_df\n",
    "team_mapping = {\n",
    "    \"NYA\": \"NYY\", \"SDN\": \"SD\", \"CHN\": \"CHC\", \"SLN\": \"STL\",\n",
    "    \"SFN\": \"SF\", \"LAN\": \"LAD\", \"TBA\": \"TB\", \"KCA\": \"KC\",\n",
    "    \"CHA\": \"CWS\", \"ANA\": \"LAA\", \"NYN\": \"NYM\"\n",
    "}\n",
    "game_df.replace(to_replace=team_mapping, inplace=True)\n",
    "\n",
    "print(\"Team names standardized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable if not exists\n",
    "if 'Home_team_won?' not in game_df.columns:\n",
    "    game_df['Home_team_won?'] = game_df['HomeRunsScore'] > game_df['VisitorRunsScored']\n",
    "\n",
    "# Create date-related columns\n",
    "if 'New_Date' not in game_df.columns and 'Date' in game_df.columns:\n",
    "    game_df['New_Date'] = pd.to_datetime(game_df['Date'].astype(str), format='%Y%m%d')\n",
    "\n",
    "if 'current_year' not in game_df.columns:\n",
    "    game_df['current_year'] = game_df['New_Date'].dt.year\n",
    "\n",
    "if 'prior_year' not in game_df.columns:\n",
    "    game_df['prior_year'] = game_df['New_Date'].dt.year - 1\n",
    "\n",
    "print(f\"Target variable distribution:\")\n",
    "print(game_df['Home_team_won?'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "print(\"Applying feature engineering...\")\n",
    "\n",
    "df_featured = feature_engineer.engineer_all_features(\n",
    "    game_df, batting_df, pitching_df,\n",
    "    include_h2h=True,\n",
    "    include_situational=True,\n",
    "    include_pitcher_rolling=True,\n",
    "    include_advanced=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal features after engineering: {len(df_featured.columns)}\")\n",
    "print(f\"Total games: {len(df_featured)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature columns (excluding non-feature columns)\n",
    "feature_columns = get_feature_columns(df_featured)\n",
    "print(f\"\\nNumber of model features: {len(feature_columns)}\")\n",
    "print(\"\\nFeature categories:\")\n",
    "\n",
    "# Categorize features\n",
    "h2h_features = [f for f in feature_columns if 'h2h' in f.lower()]\n",
    "pitcher_features = [f for f in feature_columns if 'pitcher' in f.lower()]\n",
    "rolling_features = [f for f in feature_columns if any(x in f for x in ['3d_', '7d_', 'rolling'])]\n",
    "advanced_features = [f for f in feature_columns if any(x in f for x in ['pythag', 'streak', 'clutch', 'momentum'])]\n",
    "\n",
    "print(f\"  - Head-to-head features: {len(h2h_features)}\")\n",
    "print(f\"  - Pitcher features: {len(pitcher_features)}\")\n",
    "print(f\"  - Rolling average features: {len(rolling_features)}\")\n",
    "print(f\"  - Advanced stats features: {len(advanced_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split by Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom train/test split function\n",
    "def train_test_split_by_year(df, feature_cols, target_col, train_years, test_years):\n",
    "    \"\"\"\n",
    "    Split data by season for proper temporal validation.\n",
    "    \"\"\"\n",
    "    train_mask = df['current_year'].isin(train_years)\n",
    "    test_mask = df['current_year'].isin(test_years)\n",
    "    \n",
    "    X_train = df.loc[train_mask, feature_cols].copy()\n",
    "    y_train = df.loc[train_mask, target_col].values.astype(int)\n",
    "    X_test = df.loc[test_mask, feature_cols].copy()\n",
    "    y_test = df.loc[test_mask, target_col].values.astype(int)\n",
    "    \n",
    "    # Convert to numeric and handle missing values\n",
    "    X_train = X_train.apply(pd.to_numeric, errors='coerce').fillna(-1)\n",
    "    X_test = X_test.apply(pd.to_numeric, errors='coerce').fillna(-1)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_years = list(range(2010, 2018))  # 2010-2017\n",
    "test_years = [2018, 2019]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_by_year(\n",
    "    df_featured, feature_columns, 'Home_team_won?', train_years, test_years\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} games ({train_years[0]}-{train_years[-1]})\")\n",
    "print(f\"Test set: {len(X_test)} games ({test_years[0]}-{test_years[-1]})\")\n",
    "print(f\"\\nTraining home win rate: {y_train.mean():.3f}\")\n",
    "print(f\"Test home win rate: {y_test.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train GRPO Models\n",
    "\n",
    "Train all four GRPO model variants and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GRPO models\n",
    "grpo_config = GRPOConfig(\n",
    "    hidden_layers=[128, 64, 32],\n",
    "    dropout_rate=0.3,\n",
    "    learning_rate=0.001,\n",
    "    group_size=8,\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10\n",
    ")\n",
    "\n",
    "# Dictionary to store models and results\n",
    "models = {}\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRPO Classifier\n",
    "print(\"Training GRPO Classifier...\")\n",
    "grpo_clf = GRPOClassifier(grpo_config)\n",
    "grpo_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grpo_clf.predict(X_test)\n",
    "y_proba = grpo_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "models['GRPO_Classifier'] = grpo_clf\n",
    "results['GRPO_Classifier'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "}\n",
    "\n",
    "print(f\"GRPO Classifier Accuracy: {results['GRPO_Classifier']['accuracy']:.4f}\")\n",
    "print(f\"GRPO Classifier ROC-AUC: {results['GRPO_Classifier']['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRPO Ensemble\n",
    "print(\"\\nTraining GRPO Ensemble...\")\n",
    "grpo_ensemble = GRPOEnsemble(grpo_config)\n",
    "grpo_ensemble.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grpo_ensemble.predict(X_test)\n",
    "y_proba = grpo_ensemble.predict_proba(X_test)[:, 1]\n",
    "\n",
    "models['GRPO_Ensemble'] = grpo_ensemble\n",
    "results['GRPO_Ensemble'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "}\n",
    "\n",
    "print(f\"\\nGRPO Ensemble Accuracy: {results['GRPO_Ensemble']['accuracy']:.4f}\")\n",
    "print(f\"GRPO Ensemble ROC-AUC: {results['GRPO_Ensemble']['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRPO Ranking Model\n",
    "print(\"\\nTraining GRPO Ranking Model...\")\n",
    "grpo_ranking = GRPORankingModel(grpo_config)\n",
    "grpo_ranking.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grpo_ranking.predict(X_test)\n",
    "y_proba = grpo_ranking.predict_proba(X_test)[:, 1]\n",
    "\n",
    "models['GRPO_Ranking'] = grpo_ranking\n",
    "results['GRPO_Ranking'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "}\n",
    "\n",
    "print(f\"GRPO Ranking Accuracy: {results['GRPO_Ranking']['accuracy']:.4f}\")\n",
    "print(f\"GRPO Ranking ROC-AUC: {results['GRPO_Ranking']['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GRPO Betting Optimizer\n",
    "print(\"\\nTraining GRPO Betting Optimizer...\")\n",
    "grpo_betting = GRPOBettingOptimizer(grpo_config)\n",
    "grpo_betting.fit(X_train, y_train)\n",
    "\n",
    "y_pred = grpo_betting.predict(X_test)\n",
    "y_proba = grpo_betting.predict_proba(X_test)[:, 1]\n",
    "\n",
    "models['GRPO_Betting'] = grpo_betting\n",
    "results['GRPO_Betting'] = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "}\n",
    "\n",
    "print(f\"GRPO Betting Accuracy: {results['GRPO_Betting']['accuracy']:.4f}\")\n",
    "print(f\"GRPO Betting ROC-AUC: {results['GRPO_Betting']['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Comparison\n",
    "\n",
    "Compare GRPO models against traditional ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline models for comparison\n",
    "print(\"Training baseline models...\\n\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "rf.fit(X_train.values, y_train)\n",
    "rf_pred = rf.predict(X_test.values)\n",
    "rf_proba = rf.predict_proba(X_test.values)[:, 1]\n",
    "\n",
    "results['RandomForest'] = {\n",
    "    'accuracy': accuracy_score(y_test, rf_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, rf_proba)\n",
    "}\n",
    "print(f\"Random Forest Accuracy: {results['RandomForest']['accuracy']:.4f}\")\n",
    "\n",
    "# Gradient Boosting\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "gbc.fit(X_train.values, y_train)\n",
    "gbc_pred = gbc.predict(X_test.values)\n",
    "gbc_proba = gbc.predict_proba(X_test.values)[:, 1]\n",
    "\n",
    "results['GradientBoosting'] = {\n",
    "    'accuracy': accuracy_score(y_test, gbc_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, gbc_proba)\n",
    "}\n",
    "print(f\"Gradient Boosting Accuracy: {results['GradientBoosting']['accuracy']:.4f}\")\n",
    "\n",
    "# AdaBoost\n",
    "abc = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "abc.fit(X_train.values, y_train)\n",
    "abc_pred = abc.predict(X_test.values)\n",
    "abc_proba = abc.predict_proba(X_test.values)[:, 1]\n",
    "\n",
    "results['AdaBoost'] = {\n",
    "    'accuracy': accuracy_score(y_test, abc_pred),\n",
    "    'roc_auc': roc_auc_score(y_test, abc_proba)\n",
    "}\n",
    "print(f\"AdaBoost Accuracy: {results['AdaBoost']['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "comparison_df = comparison_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "colors = ['#2ecc71' if 'GRPO' in name else '#3498db' for name in comparison_df.index]\n",
    "comparison_df['accuracy'].plot(kind='barh', ax=axes[0], color=colors)\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].axvline(x=0.5, color='red', linestyle='--', label='Random Baseline')\n",
    "\n",
    "# ROC-AUC comparison\n",
    "comparison_df['roc_auc'].plot(kind='barh', ax=axes[1], color=colors)\n",
    "axes[1].set_xlabel('ROC-AUC')\n",
    "axes[1].set_title('Model ROC-AUC Comparison')\n",
    "axes[1].axvline(x=0.5, color='red', linestyle='--', label='Random Baseline')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/grpo_model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Betting Performance Analysis\n",
    "\n",
    "Evaluate models for betting ROI and profitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure betting simulation\n",
    "betting_config = BettingConfig(\n",
    "    initial_bankroll=10000.0,\n",
    "    bet_sizing='half_kelly',\n",
    "    min_edge_threshold=0.03,\n",
    "    max_bet_pct=0.10\n",
    ")\n",
    "\n",
    "betting_analyzer = BettingAnalyzer(betting_config)\n",
    "betting_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate betting performance for each GRPO model\n",
    "for model_name in ['GRPO_Classifier', 'GRPO_Ensemble', 'GRPO_Ranking', 'GRPO_Betting']:\n",
    "    model = models[model_name]\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Create predictions DataFrame\n",
    "    predictions = pd.DataFrame({\n",
    "        'home_win_prob': y_proba,\n",
    "        'Home_team_won?': y_test\n",
    "    })\n",
    "    \n",
    "    # Add synthetic odds based on model probabilities\n",
    "    predictions = add_synthetic_odds(predictions, 'home_win_prob')\n",
    "    \n",
    "    # Run simulation\n",
    "    result = betting_analyzer.simulate_betting_season(predictions)\n",
    "    betting_results[model_name] = result\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  ROI: {result['roi_pct']:.2f}%\")\n",
    "    print(f\"  Total Bets: {result['total_bets']}\")\n",
    "    print(f\"  Win Rate: {result['win_rate_pct']:.1f}%\")\n",
    "    print(f\"  Final Bankroll: ${result['final_bankroll']:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bankroll progression for each model\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for model_name, result in betting_results.items():\n",
    "    bankroll_history = result['bankroll_history']\n",
    "    ax.plot(bankroll_history, label=f\"{model_name} (ROI: {result['roi_pct']:.1f}%)\")\n",
    "\n",
    "ax.axhline(y=10000, color='black', linestyle='--', label='Initial Bankroll')\n",
    "ax.set_xlabel('Number of Bets')\n",
    "ax.set_ylabel('Bankroll ($)')\n",
    "ax.set_title('Bankroll Progression by Model')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/grpo_betting_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed betting report for best performing model\n",
    "best_betting_model = max(betting_results.items(), key=lambda x: x[1]['roi_pct'])\n",
    "print(f\"\\nBest Betting Model: {best_betting_model[0]}\")\n",
    "print(generate_betting_report(best_betting_model[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ROI Analysis by Confidence Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ROI by confidence level\n",
    "roi_analyzer = ROIAnalyzer()\n",
    "\n",
    "# Use the best GRPO model\n",
    "best_model_name = best_betting_model[0]\n",
    "best_model = models[best_model_name]\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create analysis DataFrame\n",
    "predictions = pd.DataFrame({\n",
    "    'home_win_prob': y_proba,\n",
    "    'Home_team_won?': y_test\n",
    "})\n",
    "\n",
    "confidence_analysis = roi_analyzer.analyze_roi_by_confidence(predictions)\n",
    "print(f\"\\nROI Analysis by Confidence Level ({best_model_name}):\")\n",
    "print(confidence_analysis.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy by confidence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy by confidence\n",
    "axes[0].bar(range(len(confidence_analysis)), confidence_analysis['accuracy'])\n",
    "axes[0].set_xticks(range(len(confidence_analysis)))\n",
    "axes[0].set_xticklabels(confidence_analysis['confidence_range'], rotation=45)\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy by Confidence Level')\n",
    "axes[0].axhline(y=0.5, color='red', linestyle='--')\n",
    "\n",
    "# Number of games by confidence\n",
    "axes[1].bar(range(len(confidence_analysis)), confidence_analysis['n_games'])\n",
    "axes[1].set_xticks(range(len(confidence_analysis)))\n",
    "axes[1].set_xticklabels(confidence_analysis['confidence_range'], rotation=45)\n",
    "axes[1].set_ylabel('Number of Games')\n",
    "axes[1].set_title('Games by Confidence Level')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/grpo_confidence_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Gradient Boosting (most interpretable)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': gbc.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "top_features = feature_importance.head(20)\n",
    "ax.barh(range(len(top_features)), top_features['importance'])\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Top 20 Most Important Features')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphs/grpo_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*80)\n",
    "print(\"MLB GRPO MODELS - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nPREDICTION PERFORMANCE:\")\n",
    "print(\"-\"*40)\n",
    "for name, result in sorted(results.items(), key=lambda x: x[1]['accuracy'], reverse=True):\n",
    "    print(f\"  {name}: {result['accuracy']:.4f} accuracy, {result['roc_auc']:.4f} ROC-AUC\")\n",
    "\n",
    "print(\"\\nBETTING PERFORMANCE:\")\n",
    "print(\"-\"*40)\n",
    "for name, result in sorted(betting_results.items(), key=lambda x: x[1]['roi_pct'], reverse=True):\n",
    "    print(f\"  {name}: {result['roi_pct']:.2f}% ROI, ${result['total_profit']:,.2f} profit\")\n",
    "\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(\"-\"*40)\n",
    "best_accuracy_model = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "best_roi_model = max(betting_results.items(), key=lambda x: x[1]['roi_pct'])\n",
    "print(f\"  - Best accuracy model: {best_accuracy_model[0]} ({best_accuracy_model[1]['accuracy']:.4f})\")\n",
    "print(f\"  - Best ROI model: {best_roi_model[0]} ({best_roi_model[1]['roi_pct']:.2f}%)\")\n",
    "print(f\"  - Total features engineered: {len(feature_columns)}\")\n",
    "print(f\"  - Training games: {len(X_train)}\")\n",
    "print(f\"  - Test games: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models for future use\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "for name, model in models.items():\n",
    "    with open(f'models/{name.lower()}.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "print(\"\\nModels saved to 'models/' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cross-Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on the best model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"Cross-validation results (5-fold):\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# For sklearn-compatible models\n",
    "for name, model in [('GradientBoosting', gbc), ('RandomForest', rf), ('AdaBoost', abc)]:\n",
    "    cv_scores = cross_val_score(model, X_train.values, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"{name}: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
